{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# conda install -c anaconda beautiful-soup --yes\n",
    "from bs4 import BeautifulSoup # package for parsing HTML and XML documents\n",
    "\n",
    "import csv # implements classes to read and write tabular data in CSV form\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_url = requests.get('https://en.wikipedia.org/wiki/Demographics_of_New_York_City').text\n",
    "soup = BeautifulSoup(website_url,'lxml')\n",
    "table = soup.find('table',{'class':'wikitable sortable'})\n",
    "#print(soup.prettify())\n",
    "\n",
    "headers = [header.text for header in table.find_all('th')]\n",
    "\n",
    "table_rows = table.find_all('tr')        \n",
    "rows = []\n",
    "for row in table_rows:\n",
    "   td = row.find_all('td')\n",
    "   row = [row.text for row in td]\n",
    "   rows.append(row)\n",
    "\n",
    "with open('BON2_POPULATION1.csv', 'w') as f:\n",
    "   writer = csv.writer(f)\n",
    "   writer.writerow(headers)\n",
    "   writer.writerows(row for row in rows if row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_data=pd.read_csv('BON2_POPULATION1.csv')\n",
    "Pop_data.drop(Pop_data.columns[[7,8,9,10,11]], axis=1,inplace=True)\n",
    "print('Data downloaded!')\n",
    "Pop_data.columns = Pop_data.columns.str.replace(' ', '')\n",
    "Pop_data.columns = Pop_data.columns.str.replace('\\'','')\n",
    "Pop_data.rename(columns={'Borough':'persons_sq_mi','County':'persons_sq_km'}, inplace=True)\n",
    "Pop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_data.rename(columns = {'NewYorkCitysfiveboroughsvte\\n' : 'Borough',\n",
    "                   'Jurisdiction\\n':'County',\n",
    "                   'Population\\n':'Estimate_2017', \n",
    "                   'Landarea\\n':'square_miles',\n",
    "                    'Density\\n':'square_km'}, inplace=True)\n",
    "Pop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_data['Borough']=Pop_data['Borough'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data['County']=Pop_data['County'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data['Estimate_2017']=Pop_data['Estimate_2017'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data['square_miles']=Pop_data['square_miles'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data['square_km']=Pop_data['square_km'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data['persons_sq_mi']=Pop_data['persons_sq_mi'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data['persons_sq_km']=Pop_data['persons_sq_km'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_data.loc[5:,['persons_sq_mi','persons_sq_km']] = Pop_data.loc[2:,['persons_sq_mi','persons_sq_km']].shift(1,axis=1)\n",
    "Pop_data.loc[5:,['square_km','persons_sq_mi']] = Pop_data.loc[2:,['square_km','persons_sq_mi']].shift(1,axis=1)\n",
    "Pop_data.loc[5:,['square_miles','square_km']] = Pop_data.loc[2:,['square_miles','square_km']].shift(1,axis=1)\n",
    "Pop_data.loc[5:,['Estimate_2017','square_miles']] = Pop_data.loc[2:,['Estimate_2017','square_miles']].shift(1,axis=1)\n",
    "Pop_data.loc[5:,['County','Estimate_2017']] = Pop_data.loc[2:,['County','Estimate_2017']].shift(1,axis=1)\n",
    "Pop_data.loc[5:,['Borough','County']] = Pop_data.loc[2:,['Borough','County']].shift(1,axis=1)\n",
    "Pop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_data = Pop_data.fillna('')\n",
    "Pop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Pop_data[((Pop_data.County == 'Sources: [2] and see individual borough articles'))].index\n",
    "Pop_data.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_data.to_csv('BON2_POPULATION.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_url = requests.get('https://en.wikipedia.org/wiki/New_York_City').text\n",
    "soup = BeautifulSoup(website_url,'lxml')\n",
    "table = soup.find('table',{'class':'wikitable sortable collapsible'})\n",
    "#print(soup.prettify())\n",
    "\n",
    "headers = [header.text for header in table.find_all('th')]\n",
    "\n",
    "table_rows = table.find_all('tr')        \n",
    "rows = []\n",
    "for row in table_rows:\n",
    "   td = row.find_all('td')\n",
    "   row = [row.text for row in td]\n",
    "   rows.append(row)\n",
    "\n",
    "with open('NYC_DEMO.csv', 'w') as f:\n",
    "   writer = csv.writer(f)\n",
    "   writer.writerow(headers)\n",
    "   writer.writerows(row for row in rows if row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demo_data=pd.read_csv('NYC_DEMO.csv')\n",
    "print('Data downloaded!')\n",
    "Demo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demo_data.columns\n",
    "Demo_data.rename(columns = {'2010[237]' : '2010',\n",
    "                   '1990[239]':'1990',\n",
    "                   '1970[239]':'1970', \n",
    "                   '1940[239]\\n':'1940',\n",
    "                    }, inplace=True)\n",
    "Demo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demo_data.columns\n",
    "Demo_data.columns = Demo_data.columns.str.replace(' ', '')\n",
    "Demo_data= Demo_data.replace('\\n',' ', regex=True)\n",
    "Demo_data\n",
    "Demo_data['1970'] = Demo_data['1970'].str.rstrip('[240]')\n",
    "Demo_data\n",
    "Demo_data.to_csv('BON2_DEMOGRAPHICS.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
